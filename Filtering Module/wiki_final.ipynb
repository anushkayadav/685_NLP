{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxoDyp9PPry6"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install google-generativeai\n",
        "!pip install --upgrade together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTH6A0bSILGd"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting text2num\n",
            "  Downloading text2num-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading text2num-2.5.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: text2num\n",
            "Successfully installed text2num-2.5.1\n",
            "Requirement already satisfied: wikipedia in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5567 sha256=c1e02193e5afa9ef6d2dc5c4ad03dbafdd582292091819cfe2d934fbaa8fc949\n",
            "  Stored in directory: /home/anushkayadav_umass_edu/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install text2num\n",
        "!pip install wikipedia\n",
        "!pip install word2number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oXsNR9RTRf-8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "#from datasets import load_dataset\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6eOyK5cD4AH"
      },
      "source": [
        "### CHECK GENERATED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D1JvdxCmD6eU"
      },
      "outputs": [],
      "source": [
        "df1=pd.read_pickle(\"anushka_generated_dataset.pkl\")\n",
        "df2=pd.read_pickle(\"arushi_generated_dataset_new_130.pkl\")\n",
        "df3=pd.read_pickle(\"arushi_generated_dataset_new_375.pkl\")\n",
        "df = pd.concat([df1, df2, df3], axis=0)\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dKlH3jMoD9ac",
        "outputId": "36489504-406c-450c-df49-7d3742689a4f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>W</th>\n",
              "      <th>C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>What film title does J. F. Lawton and Shawn Le...</td>\n",
              "      <td>producer</td>\n",
              "      <td>J. F. Lawton &gt; Jonathan Frederick \"J. F.\" Lawt...</td>\n",
              "      <td>First, identify the [film titles of J. F. Lawt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Jolen is a member of the race of what type of ...</td>\n",
              "      <td>superhumans</td>\n",
              "      <td>Jolen (comics) &gt;  He is a member of the Inhuma...</td>\n",
              "      <td>First, find out [Jolen affiliation in comics -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>What is the name of the album that was release...</td>\n",
              "      <td>Blackstar</td>\n",
              "      <td>Death of David Bowie &gt; On 10 January 2016, Eng...</td>\n",
              "      <td>First, find out [album released by David Bowie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>What year was the Montgomery based school that...</td>\n",
              "      <td>1867</td>\n",
              "      <td>Mabel Murphy Smythe-Haith &gt;  He later moved to...</td>\n",
              "      <td>First, determine the [school founded in Montgo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>As of 2016, which band released more albums, T...</td>\n",
              "      <td>The Classic Crime</td>\n",
              "      <td>The Last Shadow Puppets &gt;  Following a lengthy...</td>\n",
              "      <td>First, find out [albums released by The Last S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Q                  A  \\\n",
              "996   What film title does J. F. Lawton and Shawn Le...           producer   \n",
              "997   Jolen is a member of the race of what type of ...        superhumans   \n",
              "998   What is the name of the album that was release...          Blackstar   \n",
              "999   What year was the Montgomery based school that...               1867   \n",
              "1000  As of 2016, which band released more albums, T...  The Classic Crime   \n",
              "\n",
              "                                                      W  \\\n",
              "996   J. F. Lawton > Jonathan Frederick \"J. F.\" Lawt...   \n",
              "997   Jolen (comics) >  He is a member of the Inhuma...   \n",
              "998   Death of David Bowie > On 10 January 2016, Eng...   \n",
              "999   Mabel Murphy Smythe-Haith >  He later moved to...   \n",
              "1000  The Last Shadow Puppets >  Following a lengthy...   \n",
              "\n",
              "                                                      C  \n",
              "996   First, identify the [film titles of J. F. Lawt...  \n",
              "997   First, find out [Jolen affiliation in comics -...  \n",
              "998   First, find out [album released by David Bowie...  \n",
              "999   First, determine the [school founded in Montgo...  \n",
              "1000  First, find out [albums released by The Last S...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9IY8dzUEAXh",
        "outputId": "642cb736-8a34-4df7-faf0-0b8a9ada6a2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1001, 4)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-iwB9yBKkZmH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from pprint import pprint\n",
        "import wikipedia\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "from transformers import pipeline\n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "\n",
        "def wikipedia_seach(query):\n",
        "  language_code = 'en'\n",
        "\n",
        "  number_of_results = 3\n",
        "  headers = {\n",
        "    # 'Authorization': 'Bearer YOUR_ACCESS_TOKEN',\n",
        "    'User-Agent': 'YOUR_APP_NAME (YOUR_EMAIL_OR_CONTACT_PAGE)'\n",
        "  }\n",
        "\n",
        "  base_url = 'https://api.wikimedia.org/core/v1/wikipedia/'\n",
        "  endpoint = '/search/page'\n",
        "  url = base_url + language_code + endpoint\n",
        "\n",
        "  search_query = query\n",
        "  parameters = {'q': search_query, 'limit': number_of_results}\n",
        "  response = requests.get(url, headers=headers, params=parameters)\n",
        "  response = json.loads(response.text)\n",
        "\n",
        "  searched_titles=[]\n",
        "  for page in range(len(response['pages'])):\n",
        "    display_title = response['pages'][page]['title']\n",
        "    searched_titles.append(display_title)\n",
        "    response['pages'][page][\"article_url\"] = 'http://en.wikipedia.org/?curid=' + str(response['pages'][page]['id'])\n",
        "    response['pages'][page]['excerpt'] = BeautifulSoup(response['pages'][page]['excerpt']).get_text()\n",
        "\n",
        "  excerpt_text=search_query+\": \"+\"\\n\".join([page['excerpt'] for page in response['pages']])\n",
        "  if(len(searched_titles)!=0):\n",
        "    #go to page whose title+ excerpt is most similar\n",
        "    # Encode search query\n",
        "    query_embedding = sentence_model.encode(search_query)\n",
        "    page_texts = [page['title'] + \": \" + page['excerpt'] for page in response['pages']]\n",
        "    page_embeddings = sentence_model.encode(page_texts)\n",
        "    # Calculate similarity scores for each page\n",
        "    similarities = cosine_similarity([query_embedding], page_embeddings)[0]\n",
        "\n",
        "    title_score_dict = dict(zip(searched_titles, similarities))\n",
        "\n",
        "    # Get the index of the page with the highest similarity\n",
        "    max_similarity_index = similarities.argmax()\n",
        "\n",
        "    # Output the page id with the highest similarity\n",
        "    max_similarity_page_id = response['pages'][max_similarity_index]['id']\n",
        "\n",
        "    try:\n",
        "      a=wikipedia.page(pageid=max_similarity_page_id)\n",
        "      full_content=a.content\n",
        "      return title_score_dict,excerpt_text,full_content\n",
        "    except:\n",
        "      return {},\"\",\"\"\n",
        "  else:\n",
        "    return {},\"\",\"\"\n",
        "\n",
        "def QA(ques, context):\n",
        "  result = question_answerer(question=ques,context=context)\n",
        "  return result['answer'], result['score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "from text_to_num import text2num\n",
        "\n",
        "url = \"https://google.serper.dev/search\"\n",
        "\n",
        "def google_search(query):\n",
        "  payload = json.dumps({\n",
        "  \"q\": query\n",
        "})\n",
        "  headers = {\n",
        "  'X-API-KEY': '44218db214e14ddca03d1a19c15af17344e35dcf',\n",
        "  'Content-Type': 'application/json'\n",
        "  }\n",
        "  excerpt_text = \"\"\n",
        "  full_content = \"\"\n",
        "  most_similar_snippets = \"\"\n",
        "  title_score_dict = {}\n",
        "\n",
        "\n",
        "  try:\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    response = json.loads(response.text)\n",
        "    #print(response)\n",
        "    if 'answerBox' in response:\n",
        "      if 'snippetHighlighted' in response['answerBox']:\n",
        "        excerpt_text = response['answerBox']['snippetHighlighted'][0]\n",
        "      elif 'answer' in response['answerBox']:\n",
        "        excerpt_text = response['answerBox']['answer']\n",
        "      else:\n",
        "        excerpt_text = response['answerBox']['snippet']\n",
        "      \n",
        "    else:\n",
        "      #print(\"HERE\")\n",
        "      full_content = [results['title'] + \": \" + results['snippet'] for results in response['organic']]\n",
        "\n",
        "    if full_content:\n",
        "      query_embedding = sentence_model.encode(query)\n",
        "      snippet_embeddings = sentence_model.encode(full_content)\n",
        "      similarities = cosine_similarity([query_embedding], snippet_embeddings)[0]\n",
        "      snippets_greater_than_70 = np.where(similarities > 0.50)[0]\n",
        "      most_similar_snippets = \" \".join([full_content[index] for index in snippets_greater_than_70])\n",
        "\n",
        "    return title_score_dict,excerpt_text,most_similar_snippets\n",
        "  except:\n",
        "    return {},\"\",\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tyaqVXrEB5Z"
      },
      "outputs": [],
      "source": [
        "## CHECKING STEP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrNL5HjLEEi4"
      },
      "outputs": [],
      "source": [
        "# first level scoring on wikipedia search\n",
        "#is wikipedia top3 matching the titles in W\n",
        "#if answer not found in excerpts , go to main wikipedia page contet whose title/excerpt matches highest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A47emrf4bssU",
        "outputId": "1227d5fb-3ca6-4fb6-f9fc-91fc32e925de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
            "Allie Goertz > Allison Beth \"Allie\" Goertz (born March 2, 1991) is an American musician. | Allie Goertz >  Goertz is known for her satirical songs based on various pop culture topics. | Allie Goertz >  Her videos are posted on YouTube under the name of Cossbysweater. | Milhouse Van Houten > Milhouse Mussolini van Houten is a fictional character featured in the animated television series \"The Simpsons\", voiced by Pamela Hayden, and created by Matt Groening who named the character after President Richard Nixon's middle name.\n",
            "First, identify [Matt Groening named Milhouse after -Wiki-> y1]. The name is [y1 -QA(Who did Matt Groening name Milhouse after?)-> y2]. The answer is y2.\n",
            "President Richard Nixon\n"
          ]
        }
      ],
      "source": [
        "row=2\n",
        "print(df.iloc[row]['Q'])\n",
        "print(df.iloc[row]['W'])\n",
        "print(df.iloc[row]['C'])\n",
        "print(df.iloc[row]['A'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bxrvWvH7Sa31"
      },
      "outputs": [],
      "source": [
        "def process_list_tuple(rep): ## DECISION HERE\n",
        "  if(str(type(rep[0]))!=\"<class 'tuple'>\"): #if wikipedia article comes as in query, return empty string as it is anyways invalid\n",
        "    return \"\"\n",
        "  #for QA result, best score string returned\n",
        "  non_empty_rep = [item for item in rep if item]  # Filter out empty tuples\n",
        "  if non_empty_rep:\n",
        "    rep = max(non_empty_rep, key=lambda x: x[1])[0]  # Get the string with the highest score\n",
        "  else:\n",
        "    rep = \"\"  # If all tuples are empty, return an empty string\n",
        "  return str(rep)\n",
        "\n",
        "def get_variables(text):\n",
        "  matches = re.findall(r'\\[(.*?)\\]', text)\n",
        "  variables={}\n",
        "  wiki_searches=[]\n",
        "  errors=[]\n",
        "  variables['all_eqs']=[]\n",
        "\n",
        "  def replace_variable(match):\n",
        "    #print(variables)\n",
        "    rep = variables.get(match.group(), match.group())\n",
        "    if isinstance(rep, list):\n",
        "        rep=process_list_tuple(rep)\n",
        "    return rep\n",
        "\n",
        "  for instruction in matches:\n",
        "    #print('at ', instruction)\n",
        "    #print(variables)\n",
        "    #---WIKI TOOL---\n",
        "    if(instruction.find(\"-Wiki\")!=-1):\n",
        "      query,ans=instruction.split(\" -Wiki-> \") #for in query variables\n",
        "      in_query_var=re.search(r'y\\d+', query)\n",
        "      if(in_query_var):\n",
        "        query=re.sub(r'y\\d+', replace_variable, query)\n",
        "\n",
        "      title_score_dict,excerpt_text,full_content=google_search(query)\n",
        "      #forcing wiki here\n",
        "      #excerpt_text=''\n",
        "      #full_content=''\n",
        "      if not excerpt_text and not full_content:\n",
        "        print('WIKI SEARCH')\n",
        "        title_score_dict,excerpt_text,full_content=wikipedia_seach(query)#wikipedia search\n",
        "        if title_score_dict: #SAVING IN VARIABLES HERE\n",
        "          #print('HERE updating varibale of wiki answer',ans.strip())\n",
        "          variables[ans.strip()] = [excerpt_text, full_content]#replaces the answer variable with extracted content of wiki\n",
        "          wiki_searches.append(title_score_dict)\n",
        "        else:\n",
        "          errors.append(f\"Error: Wikipedia search unsuccessful for query: '{query}'\")\n",
        "          break\n",
        "      else:\n",
        "        variables[ans.strip()] = [excerpt_text, full_content]#replaces the answer variable with extracted content of google \n",
        "\n",
        "    #---QA TOOL---\n",
        "    elif(instruction.find(\"-QA\")!=-1):\n",
        "      q,a=instruction.split(\"->\")\n",
        "      result = re.search(r'\\((.*?)\\)', q)\n",
        "      if result:\n",
        "          question = result.group(1)\n",
        "      else:\n",
        "          errors.append(\"Error in finding question\")\n",
        "          continue\n",
        "\n",
        "      pre=q.split(\" -QA\")[0]\n",
        "      context_exc=\"\"\n",
        "      full_context=\"\"\n",
        "      if (pre.find(\"+\")!=-1): #if multiple contexts need to be appended\n",
        "\n",
        "        vars=pre.split(\"+\")\n",
        "        for i in vars:\n",
        "          if(i.strip() in variables.keys()):\n",
        "            if((str(type(variables[i.strip()][0]))!=\"<class 'tuple'>\")):\n",
        "              context_exc=variables[i.strip()][0] #if variable came from wiki search, it would be a string else it would be a tuple\n",
        "              full_context=variables[i.strip()][1]\n",
        "            else:\n",
        "              t=process_list_tuple(variables[i.strip()])\n",
        "              #print('-----t',t)\n",
        "              context_exc=t #if variable came from wiki search, it would be a string else it would be a tuple\n",
        "              full_context=t\n",
        "          else:\n",
        "            errors.append(f\"Error: Context variable '{i.strip()}' not found\")\n",
        "            break\n",
        "\n",
        "      else: #single context QA\n",
        "        context_var=pre.strip()\n",
        "        if context_var in variables.keys():\n",
        "          if((str(type(variables[context_var][0]))!=\"<class 'tuple'>\")):\n",
        "            context_exc=variables[context_var][0] #if variable came from wiki search, it would be a string else it would be a tuple\n",
        "            full_context=variables[context_var][1]\n",
        "          else:\n",
        "            t=process_list_tuple(variables[context_var])\n",
        "            context_exc=t #if variable came from wiki search, it would be a string else it would be a tuple\n",
        "            full_context=t\n",
        "        else:\n",
        "          errors.append(f\"Error: Context variable '{context_var}' not found\")\n",
        "          break\n",
        "\n",
        "      #cant send empty context to QA model\n",
        "      if(context_exc!=\"\"):\n",
        "        #print(question,context_exc)\n",
        "        execerpt_run=QA(question, context_exc)\n",
        "        if(execerpt_run[-1]<0.75 and len(full_context)>0):\n",
        "          full_run=QA(question, full_context)\n",
        "        else:\n",
        "          full_run=()\n",
        "      else:\n",
        "        execerpt_run=()\n",
        "        if(len(full_context)>0):\n",
        "          full_run=QA(question, full_context)\n",
        "        else:\n",
        "          full_run=()\n",
        "\n",
        "\n",
        "      #print('Updating variable',a.strip())\n",
        "      variables[a.strip()]=[execerpt_run,full_run]\n",
        "\n",
        "    #----MATH TOOL----\n",
        "    else:\n",
        "      #print('going to math tool')\n",
        "      #print('at math',instruction)\n",
        "      variables['all_eqs'].append(instruction)\n",
        "      equation_variables = re.findall(r'y\\d+', instruction)\n",
        "      #print('found varibales',equation_variables)\n",
        "      for var in equation_variables:\n",
        "        if(var in variables.keys()):\n",
        "          #print('on', var)\n",
        "          var_ans=process_list_tuple(variables[var])\n",
        "          #print('before process',variables[var])\n",
        "          #print('on process',var_ans)\n",
        "          numbers = re.findall(r'\\d+(?:\\.\\d+)?', var_ans)#we try searching for numbers\n",
        "          if(len(numbers)>0):\n",
        "            #print('adding equation',var, numbers)\n",
        "            variables['all_eqs'].append(var+\" = \"+str(numbers[0])) #making equation for previously retrieved numerical values ### DECISON HERE\n",
        "            errors.append(f\"WARNING: '{str(numbers)}'numbers in var ans\")\n",
        "          else:\n",
        "            try:\n",
        "              #var_ans=str(text2num(var_ans, \"en\"))\n",
        "              var_ans=str(w2n.word_to_num(var_ans))\n",
        "              numbers = re.findall(r'\\d+(?:\\.\\d+)?', var_ans)#we try searching for numbers\n",
        "              if(len(numbers)>0):\n",
        "                #print('adding equation',var, numbers)\n",
        "                variables['all_eqs'].append(var+\" = \"+str(numbers[0])) #making equation for previously retrieved numerical values ### DECISON HERE\n",
        "                errors.append(f\"WARNING: '{str(numbers)}'numbers in var ans\")\n",
        "            except:\n",
        "              print('var ans is not a numeric',var_ans)\n",
        "              errors.append(f\"Error: '{var_ans}' not numeric\")\n",
        "\n",
        "\n",
        "  return variables,wiki_searches,errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_all(equations):\n",
        "  ans_dict={}\n",
        "  list_eq=[]\n",
        "  for i in equations:\n",
        "    eq=sympify(\"Eq(\" + i.replace(\"=\", \",\") + \")\")\n",
        "    list_eq.append(eq)\n",
        "  #print(list_eq)\n",
        "  result =solve(list_eq)\n",
        "  #print(result)\n",
        "  if(str(type(result))==\"<class 'list'>\"):\n",
        "    result=result[0]\n",
        "  for i in result.keys():\n",
        "      ans_dict[str(i)]=result[i]\n",
        "  return ans_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Oberoi family is part of a hotel company that has a head office in what city?\n",
            "Search [Oberoi Group head office city -Wiki-> y1]. The city is [y1 -QA(The Oberoi Group head office is in which city?)-> y2]. The answer is y2.\n",
            "Delhi\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Oberoi Group head office city -Wiki-> y1',\n",
              " 'y1 -QA(The Oberoi Group head office is in which city?)-> y2']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "row=1\n",
        "print(df.iloc[row]['Q'])\n",
        "print(df.iloc[row]['C'])\n",
        "print(df.iloc[row]['A'])\n",
        "text=df.iloc[row]['C']\n",
        "matches = re.findall(r'\\[(.*?)\\]', text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "v,w,e=get_variables(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'all_eqs': [],\n",
              " 'y1': ['New Delhi, India', ''],\n",
              " 'y2': [('New Delhi', 0.984399676322937), ()]}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wluHd850lQmO"
      },
      "outputs": [],
      "source": [
        "processed=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqkNfQ9Roeca",
        "outputId": "14df2e48-83cf-40b7-e7d5-5892e34d2d13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "g4hqvhJebkLI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▋         | 64/1001 [01:26<24:40,  1.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 87/1001 [02:02<19:02,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 94/1001 [02:12<20:00,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 116/1001 [02:42<21:32,  1.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 158/1001 [03:27<13:07,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 169/1001 [03:40<17:32,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 193/1001 [04:23<18:21,  1.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██▏       | 215/1001 [04:57<23:19,  1.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 292/1001 [06:46<19:13,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 305/1001 [07:03<13:44,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 328/1001 [07:35<12:04,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 359/1001 [08:22<13:07,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 377/1001 [08:43<13:08,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 385/1001 [08:56<14:18,  1.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 408/1001 [09:33<21:12,  2.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 420/1001 [09:55<13:35,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            " 49%|████▉     | 490/1001 [11:26<10:27,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 503/1001 [11:48<10:03,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 524/1001 [12:21<14:32,  1.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 551/1001 [12:58<09:35,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 635/1001 [15:08<06:50,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▊   | 688/1001 [16:12<08:37,  1.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 705/1001 [16:43<09:41,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▋  | 765/1001 [18:04<06:58,  1.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WIKI SEARCH\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 807/1001 [19:04<05:38,  1.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "var ans is not a numeric Eugene Lee\n",
            "Ragtime\n",
            "var ans is not a numeric Alexandra Shiva\n",
            "var ans is not a numeric Alexandra Shiva\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [23:30<00:00,  1.41s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for row in tqdm(range(df.shape[0])):\n",
        "  row_d=df.iloc[row].to_dict()\n",
        "  try:\n",
        "    answer_var=row_d['C'].lower().split(\"answer is\")[-1].replace(\".\",\"\").strip()\n",
        "    variables, wiki_searches, errors = get_variables(row_d['C'])\n",
        "    if(len(variables['all_eqs'])>0):\n",
        "      #solve eqs\n",
        "      try:\n",
        "        variables.update(solve_all(variables['all_eqs']))\n",
        "      except:\n",
        "        errors.append(\"Equaltions not solvable\")\n",
        "    if(answer_var in variables.keys()):\n",
        "      answer=variables[answer_var]\n",
        "    else:\n",
        "      answer=\"not found\"\n",
        "  except:\n",
        "    variables=[]\n",
        "    wiki_searches=[]\n",
        "    errors=['Outer Error Handling']\n",
        "    answer=\"not found\"\n",
        "\n",
        "  row_d['Variables'] = variables\n",
        "  row_d['Searches'] = wiki_searches\n",
        "  row_d['Errors'] = errors\n",
        "  row_d['Generated_answer'] = answer\n",
        "  processed.append(row_d)\n",
        "\n",
        "  # Save to pickle file every 50 rows\n",
        "  if row % 50 == 0 and row != 0:\n",
        "      processed_df = pd.DataFrame(processed)\n",
        "      #processed_df.to_pickle(f'processed_rows_wiki2.pkl')\n",
        "\n",
        "\n",
        "# Save any remaining processed rows to a pickle file\n",
        "if(processed):\n",
        "    processed_df = pd.DataFrame(processed)\n",
        "    #processed_df.to_pickle(f'processed_rows_final_wiki2.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " What nationality was James Henry Miller's wife?\n",
            "Find out [James Henry Miller wife nationality -Wiki-> y1]. Her nationality is [y1 -QA(What nationality was James Henry Miller's wife?)-> y2]. The answer is y2.\n",
            "American\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['James Henry Miller wife nationality -Wiki-> y1',\n",
              " \"y1 -QA(What nationality was James Henry Miller's wife?)-> y2\"]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "row=3\n",
        "print(df.iloc[row]['Q'])\n",
        "print(df.iloc[row]['C'])\n",
        "print(df.iloc[row]['A'])\n",
        "text=df.iloc[row]['C']\n",
        "matches = re.findall(r'\\[(.*?)\\]', text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'all_eqs': [],\n",
              " 'y1': ['',\n",
              "  'Henry Miller - Wikipedia: In 1967, Miller married his fifth wife, Japanese born singer Hoki Tokuda (ja:ホキ徳田). James Henry Miller (1860–1935) • FamilySearch: James Henry Miller was born on 20 June 1860, in Alabama, United States as the son of Mr Miller and Mrs Miller. He married Nancy Ophelia Moore on 10 April ... James Henry Miller Obituary - Ventura County Star: James (Jim) H. Miller, loving husband to Carol Miller, passed away on February 15th. He was born in 1932 to James and Esther Miller. Cpl. James Henry Miller (USA) - Geni.com: James was the son of Joseph Miller and Nancy Jane Miller. He married Martha Simon on Mar 6 862 in Buckhannon, West Virginia. This marriage ... Obituary for James Henry Miller | Henline - Hughes Funeral Home: James Henry Miller, age 71 of Hwy 226 North, Bakersville, NC passed away August 25, 2014 at his home. He was a native of Mitchell County, NC and ... James Henry Miller living in Southwark, London,Surrey in 1911 ...: Martha · Miller · Wife · Married · 38 · 1873 · London Southwark. Henry Miller - Historical records and family trees - MyHeritage: Henry married Francenia Miller (born Morris) on month day 1847, at age 32 in marriage place, Indiana. Francenia was born on month day 1825, in birth place, ... James H. Miller - e-WV: In 1882, Miller married Jane Tompkins Miller of Gauley Bridge. His History of Summers County, published in 1908, was a monumental work ...'],\n",
              " 'y2': [(), ('Japanese', 0.6979333162307739)]}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.iloc[3]['Variables']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>W</th>\n",
              "      <th>C</th>\n",
              "      <th>Variables</th>\n",
              "      <th>Searches</th>\n",
              "      <th>Errors</th>\n",
              "      <th>Generated_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which magazine was started first Arthur's Maga...</td>\n",
              "      <td>Arthur's Magazine</td>\n",
              "      <td>Arthur's Magazine &gt; Arthur's Magazine (1844–18...</td>\n",
              "      <td>First, search [Arthur's Magazine -Wiki-&gt; y1]. ...</td>\n",
              "      <td>{'all_eqs': [], 'y1': ['', 'Arthur's Magazine ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[(), (First For Women - Health, Diet, Beauty T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Oberoi family is part of a hotel company t...</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Oberoi family &gt; The Oberoi family is an Indian...</td>\n",
              "      <td>Search [Oberoi Group head office city -Wiki-&gt; ...</td>\n",
              "      <td>{'all_eqs': [], 'y1': ['New Delhi, India', '']...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[(New Delhi, 0.984399676322937), ()]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Musician and satirist Allie Goertz wrote a son...</td>\n",
              "      <td>President Richard Nixon</td>\n",
              "      <td>Allie Goertz &gt; Allison Beth \"Allie\" Goertz (bo...</td>\n",
              "      <td>First, identify [Matt Groening named Milhouse ...</td>\n",
              "      <td>{'all_eqs': [], 'y1': ['U.S. president Richard...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[(Richard Nixon, 0.982280969619751), ()]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What nationality was James Henry Miller's wife?</td>\n",
              "      <td>American</td>\n",
              "      <td>Peggy Seeger &gt; Margaret \"Peggy\" Seeger (born J...</td>\n",
              "      <td>Find out [James Henry Miller wife nationality ...</td>\n",
              "      <td>{'all_eqs': [], 'y1': ['', 'Henry Miller - Wik...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[(), (Japanese, 0.6979333162307739)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cadmium Chloride is slightly soluble in this c...</td>\n",
              "      <td>alcohol</td>\n",
              "      <td>Cadmium chloride &gt;  It is a hygroscopic solid ...</td>\n",
              "      <td>First, search [Cadmium Chloride chemical it is...</td>\n",
              "      <td>{'all_eqs': [], 'y1': ['slightly soluble in al...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[(alcohol, 0.9992592334747314), ()]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Q                        A  \\\n",
              "0  Which magazine was started first Arthur's Maga...        Arthur's Magazine   \n",
              "1  The Oberoi family is part of a hotel company t...                    Delhi   \n",
              "2  Musician and satirist Allie Goertz wrote a son...  President Richard Nixon   \n",
              "3    What nationality was James Henry Miller's wife?                 American   \n",
              "4  Cadmium Chloride is slightly soluble in this c...                  alcohol   \n",
              "\n",
              "                                                   W  \\\n",
              "0  Arthur's Magazine > Arthur's Magazine (1844–18...   \n",
              "1  Oberoi family > The Oberoi family is an Indian...   \n",
              "2  Allie Goertz > Allison Beth \"Allie\" Goertz (bo...   \n",
              "3  Peggy Seeger > Margaret \"Peggy\" Seeger (born J...   \n",
              "4  Cadmium chloride >  It is a hygroscopic solid ...   \n",
              "\n",
              "                                                   C  \\\n",
              "0  First, search [Arthur's Magazine -Wiki-> y1]. ...   \n",
              "1  Search [Oberoi Group head office city -Wiki-> ...   \n",
              "2  First, identify [Matt Groening named Milhouse ...   \n",
              "3  Find out [James Henry Miller wife nationality ...   \n",
              "4  First, search [Cadmium Chloride chemical it is...   \n",
              "\n",
              "                                           Variables Searches Errors  \\\n",
              "0  {'all_eqs': [], 'y1': ['', 'Arthur's Magazine ...       []     []   \n",
              "1  {'all_eqs': [], 'y1': ['New Delhi, India', '']...       []     []   \n",
              "2  {'all_eqs': [], 'y1': ['U.S. president Richard...       []     []   \n",
              "3  {'all_eqs': [], 'y1': ['', 'Henry Miller - Wik...       []     []   \n",
              "4  {'all_eqs': [], 'y1': ['slightly soluble in al...       []     []   \n",
              "\n",
              "                                    Generated_answer  \n",
              "0  [(), (First For Women - Health, Diet, Beauty T...  \n",
              "1               [(New Delhi, 0.984399676322937), ()]  \n",
              "2           [(Richard Nixon, 0.982280969619751), ()]  \n",
              "3               [(), (Japanese, 0.6979333162307739)]  \n",
              "4                [(alcohol, 0.9992592334747314), ()]  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df.to_pickle(\"wikipedia_processed1000.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NmA861q368EH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anushkayadav_umass_edu/.conda/envs/llmal/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the additional functions for similarity checks\n",
        "def exact_match(text1, text2):\n",
        "    return text1 == text2\n",
        "\n",
        "def fuzzy_match(text1, text2):\n",
        "    ratio = fuzz.partial_ratio(text1.lower(), text2.lower())\n",
        "    return ratio\n",
        "\n",
        "def semantic_similarity(text1, text2):\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embeddings1 = model.encode(text1, convert_to_tensor=True)\n",
        "    embeddings2 = model.encode(text2, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "    return cosine_scores.item()\n",
        "\n",
        "def semantic_similarity_phrase_bert(text1, text2):\n",
        "    phrase_list = [text1, text2]\n",
        "    model = SentenceTransformer('whaleloops/phrase-bert')\n",
        "    phrase_embs = model.encode(phrase_list)\n",
        "    [p1, p2] = phrase_embs\n",
        "    cos_sim = nn.CosineSimilarity(dim=0)\n",
        "    return cos_sim(torch.tensor(p1), torch.tensor(p2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TPba8C3qNkhr"
      },
      "outputs": [],
      "source": [
        "# Update the check_answer function to include new similarity checks\n",
        "def check_answer(row):\n",
        "    gen = row['Generated_answer']\n",
        "    act = row['A']\n",
        "    all_vars = row['Variables']\n",
        "\n",
        "    if exact_match(gen, act):\n",
        "        return True\n",
        "    if str(gen).find(str(act)) != -1:\n",
        "        return True\n",
        "    if str(gen) == str(act):\n",
        "        return True\n",
        "    if fuzzy_match(str(gen), act) > 80:  # You can adjust the threshold for fuzzy matching\n",
        "        return True\n",
        "    if semantic_similarity(str(gen), act) > 0.6:  # Adjust the threshold for semantic similarity\n",
        "        return True\n",
        "    if semantic_similarity_phrase_bert(str(gen), act) > 0.6:  # Adjust the threshold for phrase BERT similarity\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [18:28<00:00,  1.11s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming 'processed_df' is your DataFrame\n",
        "tqdm.pandas()  # Initialize tqdm with pandas\n",
        "\n",
        "# Apply the check_answer function with a progress bar\n",
        "processed_df['is_final_in_solved'] = processed_df.progress_apply(check_answer, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Eri_hhN_Nkfh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([False,  True])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['is_final_in_solved'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "PUWcQHc1Nkcp"
      },
      "outputs": [],
      "source": [
        "# Count the rows where final_answer is present in solved_answer list\n",
        "count_valid = processed_df[processed_df['is_final_in_solved']].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "lrShiw8oO03g"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "332"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_valid "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "IE_pgMA0O0y0"
      },
      "outputs": [],
      "source": [
        "processed_df.to_pickle(\"wikipdia1000_validated.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "wWi55eEBO0uY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(921, 9)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the words to filter for\n",
        "error_words = ['Error']\n",
        "\n",
        "# Function to check if any word in a list is in the specified error_words\n",
        "def has_error_words(lst):\n",
        "    return not any(word in str(lst) for word in error_words)\n",
        "\n",
        "# Apply the function to filter rows\n",
        "non_error_df = processed_df[processed_df['Errors'].apply(has_error_words)]\n",
        "non_error_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(321, 9)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solved_rows_df = processed_df[processed_df['is_final_in_solved'] == True]\n",
        "solved_rows_df = solved_rows_df[solved_rows_df['Errors'].apply(has_error_words)]\n",
        "solved_rows_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify rows in non_error_df that are not in solved_rows_df\n",
        "unique_non_error_df = non_error_df[~non_error_df.index.isin(solved_rows_df.index)]\n",
        "\n",
        "# Select 40 rows from the identified rows\n",
        "selected_rows = unique_non_error_df.sample(n=200, random_state=42)  # Adjust the sampling criteria as needed\n",
        "\n",
        "# Concatenate selected rows with solved_rows_df\n",
        "concatenated_df = pd.concat([solved_rows_df, selected_rows])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(521, 9)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_df.to_pickle(\"../all_data_finals/atFinal-wikitool.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTO-7XwGWuFv"
      },
      "source": [
        "# EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY9zW2B24IQD",
        "outputId": "6955842b-dca0-4ce1-91fb-2fdfec90f242"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['type of script used in autographs -Wiki-> y1',\n",
              " 'y1 -QA(What script was used in autographs?)-> y2',\n",
              " 'Cuneiform script invented by-Wiki-> y3',\n",
              " 'y3 -QA(Who invented cuneiform script?)-> y4']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "row=95\n",
        "text=df.iloc[row]['C']\n",
        "matches = re.findall(r'\\[(.*?)\\]', text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arvxBul74xBc",
        "outputId": "0b40f143-0a58-4afd-8957-cf84446b1ed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "at isnt Henri Leconte Grand Slam titles  -Wiki-> y1\n",
            "at isnt Jonathan Stark Grand Slam titles  -Wiki-> y2\n",
            "at isnt y1+y2 -QA(Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?)-> y3\n"
          ]
        }
      ],
      "source": [
        "def replace_variable(match):\n",
        "    return variables.get(match.group(), match.group())\n",
        "\n",
        "variables={}\n",
        "wiki_searches=[]\n",
        "for instruction in matches:\n",
        "  #print(\"at isnt\",instruction,)\n",
        "  if(instruction.find(\"-Wiki\")!=-1):\n",
        "    query,ans=instruction.split(\" -Wiki-> \")\n",
        "    #for in query variables\n",
        "    in_query_var=re.search(r'y\\d+', query)\n",
        "    if(in_query_var):\n",
        "      print(\"FOUND INQUERY VAR\")\n",
        "      query=re.sub(r'y\\d+', replace_variable, query)\n",
        "    #wikipedia search\n",
        "    title_score_dict,excerpt_text,full_content=wikipedia_seach(query)\n",
        "    #replaces the answer variable with extracted content\n",
        "    variables[ans.strip()]= [excerpt_text,full_content]\n",
        "    wiki_searches.append(title_score_dict)\n",
        "\n",
        "  if(instruction.find(\"-QA\")!=-1):\n",
        "    q,a=instruction.split(\"->\")\n",
        "    result = re.search(r'\\((.*?)\\)', text)\n",
        "    if result:\n",
        "        question = result.group(1)\n",
        "    else:\n",
        "        print(\"Error in finding question\")\n",
        "        continue\n",
        "\n",
        "    pre=q.split(\" -QA\")[0]\n",
        "    if (pre.find(\"+\")!=-1):\n",
        "      vars=pre.split(\"+\")\n",
        "      context_exc=\"\"\n",
        "      full_context=\"\"\n",
        "      for i in vars:\n",
        "        if(i.strip() in variables.keys()):\n",
        "          context_exc=context_exc+variables[i.strip()][0]\n",
        "          full_context=full_context+variables[i.strip()][1]\n",
        "        else:\n",
        "          print('error in finding context variable')\n",
        "          continue\n",
        "\n",
        "    else:\n",
        "      context_var=pre.strip()\n",
        "      if context_var in variables.keys():\n",
        "        context_exc=variables[context_var][0]\n",
        "        full_context=variables[context_var][1]\n",
        "      else:\n",
        "        print('error in finding context variable')\n",
        "        continue\n",
        "\n",
        "    execerpt_run=QA(question, context_exc)\n",
        "    if(execerpt_run[-1]<0.75):\n",
        "      full_run=QA(question, full_context)\n",
        "    else:\n",
        "      full_run=()\n",
        "    variables[a.strip()]=[execerpt_run,full_run]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK_mL1CEuMe3"
      },
      "outputs": [],
      "source": [
        "variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIta5rcwo-Sg",
        "outputId": "ecdc5b7c-1e27-4493-88bf-3fd8f330477b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Henri Leconte': 0.6307155,\n",
              "  'Grand Slam Cup': 0.3890447,\n",
              "  \"Open Era tennis records – Men's singles\": 0.378835},\n",
              " {'Jonathan Stark (tennis)': 0.7212882,\n",
              "  'Serena Williams': 0.47569674,\n",
              "  \"List of Grand Slam men's doubles champions\": 0.4702412}]"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wiki_searches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ME1-jFtAfA",
        "outputId": "0add0a03-6830-4327-be4c-36e9dc5b3c2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Jonathan Stark (tennis)', 'Jonathan Stark (tennis)', 'Henri Leconte']"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "true_wiki_pages=[i.split(\">\")[0].strip() for i in df.iloc[5]['W'].split(\"|\")]\n",
        "true_wiki_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vqlfwj7XAJW",
        "outputId": "4e32bffc-2152-4ad5-d2f6-db78978e6f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37402\n"
          ]
        }
      ],
      "source": [
        "query=\"Number of legs in chicken\"\n",
        "t,e,f=wikipedia_seach(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOmZKXw9WCaU"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm0WiZRvz5fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd3n3OKpz5dY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39PRz7sa1yhx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE936kuW1yfu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdq5KVtz1rOY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bezt9Qvo1t9A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "V-Gy0GP9kJHi"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
